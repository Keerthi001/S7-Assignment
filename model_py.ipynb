{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.py",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keerthi001/S7-Assignment/blob/main/model_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV1_6gkxpEn8"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "class BatchNorm(nn.BatchNorm2d):\r\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, weight=True, bias=True):\r\n",
        "        super().__init__(num_features, eps=eps, momentum=momentum)\r\n",
        "        self.weight.data.fill_(1.0)\r\n",
        "        self.bias.data.fill_(0.0)\r\n",
        "        self.weight.requires_grad = weight\r\n",
        "        self.bias.requires_grad = bias\r\n",
        "\r\n",
        "\r\n",
        "class GhostBatchNorm(BatchNorm):\r\n",
        "    def __init__(self, num_features, num_splits, **kw):\r\n",
        "        super().__init__(num_features, **kw)\r\n",
        "        self.num_splits = num_splits\r\n",
        "        self.register_buffer('running_mean', torch.zeros(\r\n",
        "            num_features * self.num_splits))\r\n",
        "        self.register_buffer('running_var', torch.ones(\r\n",
        "            num_features * self.num_splits))\r\n",
        "\r\n",
        "    def train(self, mode=True):\r\n",
        "        # lazily collate stats when we are going to use them\r\n",
        "        if (self.training is True) and (mode is False):\r\n",
        "            self.running_mean = torch.mean(self.running_mean.view(self.num_splits, self.num_features), dim=0).repeat(\r\n",
        "                self.num_splits)\r\n",
        "            self.running_var = torch.mean(self.running_var.view(self.num_splits, self.num_features), dim=0).repeat(\r\n",
        "                self.num_splits)\r\n",
        "        return super().train(mode)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        N, C, H, W = input.shape\r\n",
        "        if self.training or not self.track_running_stats:\r\n",
        "            return F.batch_norm(\r\n",
        "                input.view(-1, C * self.num_splits, H,\r\n",
        "                           W), self.running_mean, self.running_var,\r\n",
        "                self.weight.repeat(self.num_splits), self.bias.repeat(\r\n",
        "                    self.num_splits),\r\n",
        "                True, self.momentum, self.eps).view(N, C, H, W)\r\n",
        "        else:\r\n",
        "            return F.batch_norm(\r\n",
        "                input, self.running_mean[:self.num_features], self.running_var[:self.num_features],\r\n",
        "                self.weight, self.bias, False, self.momentum, self.eps)\r\n",
        "\r\n",
        "\r\n",
        "class Net(nn.Module):\r\n",
        "    def __init__(self, is_GBN=False, gbn_splits=2, in_c = 1):\r\n",
        "        super(Net, self).__init__()\r\n",
        "        \r\n",
        "        # CONVOLUTION BLOCK 1\r\n",
        "        self.convblock1 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=in_c, out_channels=64,\r\n",
        "                      kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(64, gbn_splits) if is_GBN else nn.BatchNorm2d(64),\r\n",
        "            \r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64,\r\n",
        "                      kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(64, gbn_splits) if is_GBN else nn.BatchNorm2d(64),\r\n",
        "            \r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64,\r\n",
        "                      kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(64, gbn_splits) if is_GBN else nn.BatchNorm2d(64)\r\n",
        "        )  # input_size = 28 output_size = 26 receptive_field = 3\r\n",
        "\r\n",
        "        # TRANSITION BLOCK 1\r\n",
        "        self.pool1 = nn.Sequential(\r\n",
        "            nn.MaxPool2d(2, 2))# input_size = 24 output_size = 12 receptive_field =\r\n",
        "\r\n",
        "        # CONVOLUTION BLOCK 2 DSConnv\r\n",
        "        self.convblock2 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64,\r\n",
        "                      kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(64, gbn_splits) if is_GBN else nn.BatchNorm2d(64),\r\n",
        "            \r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64,\r\n",
        "                      kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(64, gbn_splits) if is_GBN else nn.BatchNorm2d(64),\r\n",
        "            \r\n",
        "            \r\n",
        "            nn.Conv2d(in_channels=64, out_channels=64,\r\n",
        "                      kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(64, gbn_splits) if is_GBN else nn.BatchNorm2d(64)\r\n",
        "        )  # input_size = 26 output_size = 24 receptive_field = 5\r\n",
        "\r\n",
        "        # TRANSITION BLOCK 2\r\n",
        "        self.pool2 = nn.Sequential(\r\n",
        "            nn.MaxPool2d(2, 2))  # input_size = 24 output_size = 12 receptive_field =\r\n",
        "\r\n",
        "        # CONVOLUTION BLOCK 3 Dilated\r\n",
        "        self.convblock3 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=64, out_channels=128,\r\n",
        "                      kernel_size=(3, 3), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(128, gbn_splits) if is_GBN else nn.BatchNorm2d(128),\r\n",
        "            \r\n",
        "            nn.Conv2d(in_channels=128, out_channels=128,\r\n",
        "                      kernel_size=(3, 3), padding=1, bias=False,dilation=2),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(128, gbn_splits) if is_GBN else nn.BatchNorm2d(128),\r\n",
        "            \r\n",
        "            nn.Conv2d(in_channels=128, out_channels=128,\r\n",
        "                      kernel_size=(3, 3), padding=1, bias=False,dilation=2),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(128, gbn_splits) if is_GBN else nn.BatchNorm2d(128)\r\n",
        "        )  # input_size = 12 output_size = 10 receptive_field = 5\r\n",
        "\r\n",
        "        # TRANSITION BLOCK 3\r\n",
        "        self.pool3 = nn.Sequential(\r\n",
        "            nn.MaxPool2d(2, 2))  # input_size = 24 output_size = 12 receptive_field =\r\n",
        "\r\n",
        "        # CONVOLUTION BLOCK 4\r\n",
        "        self.convblock4 = nn.Sequential(\r\n",
        "            nn.Conv2d(in_channels=128, out_channels=256,\r\n",
        "                      kernel_size=(1, 1), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(256, gbn_splits) if is_GBN else nn.BatchNorm2d(256),\r\n",
        "            \r\n",
        "            nn.Conv2d(in_channels=256, out_channels=256,\r\n",
        "                      kernel_size=(1, 1), padding=1, bias=False,groups=64),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(256, gbn_splits) if is_GBN else nn.BatchNorm2d(256),\r\n",
        "            \r\n",
        "            nn.Conv2d(in_channels=256, out_channels=256,\r\n",
        "                      kernel_size=(1, 1), padding=1, bias=False),\r\n",
        "            nn.ReLU(),\r\n",
        "            GhostBatchNorm(256, gbn_splits) if is_GBN else nn.BatchNorm2d(256)\r\n",
        "        )  # input_size = 12 output_size = 10 receptive_field = 5\r\n",
        "\r\n",
        "        \r\n",
        "        # OUTPUT BLOCK with GAP\r\n",
        "        self.output = nn.Sequential(\r\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\r\n",
        "            nn.Conv2d(in_channels=256, out_channels=10,\r\n",
        "                      kernel_size=(1, 1), padding=0, bias=False)\r\n",
        "        )  # input_size = 5 output_size = 1  receptive_field = 29\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.convblock1(x)\r\n",
        "        x = self.pool1(x)\r\n",
        "        x = self.convblock2(x)\r\n",
        "        x = self.pool2(x)\r\n",
        "        x = self.convblock3(x)\r\n",
        "        x = self.pool3(x)\r\n",
        "        x = self.convblock4(x)\r\n",
        "        x = self.output(x)\r\n",
        "        \r\n",
        "        x = x.view(-1, 10)\r\n",
        "        return F.log_softmax(x, dim=-1)\r\n",
        "\r\n",
        "def get_optimizer(model_parameters,loss_type):\r\n",
        "    if loss_type == \"L2\":\r\n",
        "        optimizer = optim.SGD(model_parameters, lr=0.01,\r\n",
        "                  momentum=0, weight_decay=0, nesterov=False)\r\n",
        "    else:\r\n",
        "        optimizer = optim.SGD(model_parameters, lr= 0.01, momentum=0.9)\r\n",
        "    return optimizer\r\n",
        "\r\n",
        "\r\n",
        "def run_model(model, device, optimiser, EPOCHS=1, is_L1_loss=False, is_GBN=False, gbn_splits=2):\r\n",
        "\r\n",
        "  train_losses = []\r\n",
        "  train_acc = []\r\n",
        "  test_losses = []\r\n",
        "  test_acc = []\r\n",
        "\r\n",
        "  for epoch in range(EPOCHS):\r\n",
        "      print(\"EPOCH:\", epoch+1)\r\n",
        "      train(model, device, train_loader, optimizer, epoch,\r\n",
        "            train_losses, train_acc, is_L1_loss, lamda_l1=0.0001)\r\n",
        "      test(model, device, test_loader, test_losses, test_acc)\r\n",
        "\r\n",
        "  return {'train_loss': train_losses,  'train_acc': train_acc,  'test_loss': test_losses,  'test_acc': test_acc}\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}